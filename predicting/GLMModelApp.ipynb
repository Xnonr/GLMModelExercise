{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a35dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Required Libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi import Request\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e87b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    '''\n",
    "    Returns loaded in, pre-trained Prediction, SimpleImputer and StandardScalar models\n",
    "    '''\n",
    "    \n",
    "    mdl = sm.load('../models/glm_final_model.pickle')\n",
    "    si = pickle.load(open('../models/glm_simple_imputer.pickle', 'rb'))\n",
    "    ss = pickle.load(open('../models/glm_standard_scalar.pickle', 'rb'))\n",
    "    \n",
    "    return mdl, si, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b6360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_json_to_df(obj):\n",
    "    '''\n",
    "    Returns a DataFrame containing raw JSON data\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    obj -- Raw JSON data in the data type form of a String, List or Dictionary\n",
    "    '''\n",
    "    \n",
    "    if isinstance(obj, str):\n",
    "        if obj[0] == '[' and obj[len(obj) - 1] == ']':\n",
    "            df = pd.read_json(obj, orient = 'records')\n",
    "        elif obj[0] == '{' and obj[len(obj) - 1] == '}':\n",
    "            obj = '[' + obj + ']'\n",
    "            df = pd.read_json(obj, orient = 'records')\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "    elif isinstance(obj, list):\n",
    "        df = pd.DataFrame.from_dict(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        obj = [obj]\n",
    "        df = pd.DataFrame.from_dict(obj)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e0e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_df(max_batch_size, df):\n",
    "    '''\n",
    "    Returns\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    max_batch_size -- \n",
    "    df -- \n",
    "    '''\n",
    "    \n",
    "    df_batches = []\n",
    "    \n",
    "    num_rows = len(df.index)\n",
    "    \n",
    "    if num_rows > max_batch_size:\n",
    "        \n",
    "        num_batches = ceil(num_rows / max_batch_size)\n",
    "        \n",
    "        for batch_num in range(num_batches): \n",
    "            starting_index = batch_num * max_batch_size\n",
    "            #print(f'Starting Index: {starting_index}')\n",
    "            \n",
    "            if batch_num != num_batches - 1:\n",
    "                ending_index = (batch_num + 1) * max_batch_size\n",
    "                #print(f'Ending Index: {ending_index}')\n",
    "                batch_df = df.iloc[starting_index:ending_index].copy(deep = True)\n",
    "            \n",
    "            else:\n",
    "                batch_df = df.iloc[starting_index:].copy(deep = True)\n",
    "                \n",
    "            df_batches.append(batch_df)\n",
    "    \n",
    "    else:\n",
    "        df_batches.append(df.copy(deep = True))\n",
    "        \n",
    "    return df_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411ecffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_column_variables(df):\n",
    "    '''\n",
    "    Returns a DataFrame after having transformed those columns, whose values \n",
    "        consisted of those of the String data type and representing quantitative \n",
    "        data, into values being that of the Float data type after removing any\n",
    "        non mathematically interpretable symbols \n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- A DataFrame containing raw JSON data\n",
    "    '''\n",
    "    \n",
    "    # Formats the 'x12' and 'x63' columns', consisting of the String data type, \n",
    "    #    and respectively representing first monetary then percentage values,\n",
    "    #    into the Float data type so as to be able to later on apply mathematical\n",
    "    #    work upon said columns' values\n",
    "    \n",
    "    columns_to_format = ['x12', 'x63']\n",
    "    \n",
    "    unwanted_symbols = ['$', ',', '(', ')', '%']\n",
    "    \n",
    "    for clmn in columns_to_format:\n",
    "        for usymb in unwanted_symbols:\n",
    "            if usymb != '(':\n",
    "                df[clmn] = df[clmn].str.replace(usymb, '', regex = True)\n",
    "            else:\n",
    "                df[clmn] = df[clmn].str.replace(usymb, '-', regex = True)\n",
    "        df[clmn] = df[clmn].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc361ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_df_data(si, df):\n",
    "    '''\n",
    "    Returns a DataFrame with no column variables whose data is of a qualitative nature, \n",
    "        as well as having filled in any remaining blank, NaN, NULL or otherwise missing \n",
    "        values via the usage of an imported, pre-trained SimpleImputer using a mean based \n",
    "        strategy\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    df -- A formatted DataFrame\n",
    "    '''\n",
    "    \n",
    "    # Columns containing qualitative data that are dropped ahead of the imputation phase\n",
    "    #    as the SimpleImputer cannot properly function upon them\n",
    "    qual_clmns = ['x5', 'x31', 'x81', 'x82']\n",
    "\n",
    "    df = pd.DataFrame(si.transform(df.drop(columns = qual_clmns)), \n",
    "                      columns = df.drop(columns = qual_clmns).columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3211268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df_data(ss, df):\n",
    "    '''\n",
    "    Returns a DataFrame whose column variable values have all been scaled via a \n",
    "        standardization method for the purpose of feature scaling, utilizing an\n",
    "        imported, pre-trained StandardScalar\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    df -- A formatted DataFrame without any blank, NaN, NULL or otherwise missing values\n",
    "    '''\n",
    "    \n",
    "    # Of particular interest and focus is that of the 'x12' column representing\n",
    "    #    monetary values which tend to outscale all other column variable values\n",
    "    #    by some orders of magnitude\n",
    "    \n",
    "    df = pd.DataFrame(ss.transform(df), \n",
    "                      columns = df.columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b65d9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_dummy_column_variables_new(df1, df2):\n",
    "    '''\n",
    "    Returns a DataFrame with dummy variables for those column variables consisting of\n",
    "        qualitative data, as well as whose column variables of numeric quantitative data\n",
    "        have no missing values and are scaled\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    df1 -- A DataFrame containing the original raw JSON data in order to retrieve those column\n",
    "           variable values of a qualitative nature previously dropped and must now be \n",
    "           dummified\n",
    "    df2 -- A DataFrame with no missing column variable values and whose said values have already been scaled\n",
    "    '''\n",
    "    \n",
    "    # A list of column variable names of a quantitative nature which require dummification\n",
    "    vars_to_dummify = ['x5', 'x31', 'x81', 'x82']\n",
    "    dummy_dfs = [df2]\n",
    "    \n",
    "    for var in vars_to_dummify:\n",
    "\n",
    "        var_dummy_vars_df = pd.get_dummies(df1[var], \n",
    "                                        drop_first = True, \n",
    "                                        prefix = var, \n",
    "                                        prefix_sep = '_', \n",
    "                                        dummy_na = True)\n",
    "        \n",
    "        dummy_dfs.append(var_dummy_vars_df)\n",
    "\n",
    "    df2 = pd.concat(dummy_dfs, axis = 1, sort = False)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8f337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_column_variables(ordr_clmn_names_lst, df):\n",
    "    '''\n",
    "    Returns a DataFrame containing only those column variables required by the pre-trained model\n",
    "        for predictions, filtering out from the given DataFrame only said columns\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    ordr_clmn_names_lst -- A list of DataFrame column variable names required by the pre-trained model\n",
    "    df -- The DataFrame with dummy variables, and whose quantitative values have been scaled and \n",
    "          have none missing\n",
    "    '''\n",
    "    \n",
    "    necessary_clmn_vars_set = set(ordr_clmn_names_lst)\n",
    "    avlbl_clmn_vars_set = set(df.columns)\n",
    "\n",
    "    # Depending upon the type and amount of JSON data originally having been passed in, \n",
    "    #    not all of the desired dummy variables will always be successfully generated, \n",
    "    #    necessitating their inclusion afterwards via the code below\n",
    "    if necessary_clmn_vars_set.issubset(avlbl_clmn_vars_set) == False:\n",
    "        nan_df = pd.DataFrame(np.nan, index = range(df.shape[0]), columns = ordr_clmn_names_lst)\n",
    "        df = df.combine_first(nan_df)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "    #df = df[ordr_clmn_names_lst].copy()\n",
    "    df = df[ordr_clmn_names_lst].copy(deep = True)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c64c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transform_input_data_pipeline(df, si, ss, ordr_clmn_names_lst):\n",
    "    '''\n",
    "    Returns a transformed DataFrame originally filled with raw JSON data, transforming said data via\n",
    "        imputation, to fill in any and all missing values, scaling, and the creation of dummy\n",
    "        variables for those qualitative column variables which require such action\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- A DataFrame containing raw JSON data\n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    ordr_clmn_names_lst -- A list of DataFrame column variable names required by the pre-trained model\n",
    "    '''\n",
    "    \n",
    "    if df.shape[0] < 1:\n",
    "        filter_dummy_scaled_imputed_df = pd.DataFrame()\n",
    " \n",
    "    else:\n",
    "        \n",
    "        # Resets the index to avoid concatenation issues with dummy variable DataFrames\n",
    "        df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        batch_df = df.copy(deep = True)\n",
    "        \n",
    "        formatted_df = format_df_column_variables(batch_df)\n",
    "\n",
    "        imputed_df = impute_missing_df_data(si, formatted_df)\n",
    "\n",
    "        scaled_imputed_df = scale_df_data(ss, imputed_df)\n",
    "\n",
    "\n",
    "        dummy_scaled_imputed_df = create_df_dummy_column_variables_new(df, scaled_imputed_df)\n",
    "\n",
    "\n",
    "        filter_dummy_scaled_imputed_df = filter_df_column_variables(ordr_clmn_names_lst, \n",
    "                                                                    dummy_scaled_imputed_df)\n",
    "        \n",
    "    return filter_dummy_scaled_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0733ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transform_df_batches(df_batches, si, ss, ordr_clmn_names_lst):\n",
    "    '''\n",
    "    Returns\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df_batches -- \n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    ordr_clmn_names_lst -- A list of DataFrame column variable names required by the pre-trained model\n",
    "    '''\n",
    "    \n",
    "    transformed_df_batches = []\n",
    "    \n",
    "    for batch in df_batches:\n",
    "        transformed_batch = extract_transform_input_data_pipeline(batch, si, ss, ordr_clmn_names_lst)\n",
    "        transformed_df_batches.append(transformed_batch)\n",
    "        \n",
    "    return transformed_df_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6576909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df_batches(df_batches):\n",
    "    '''\n",
    "    Returns\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df_batches -- \n",
    "    '''\n",
    "    \n",
    "    df = pd.concat(df_batches, ignore_index = True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38021796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_outcome(df, mdl, alphanum_ord_clmn_var_names_lst):\n",
    "    '''\n",
    "    Returns a JSON message containing either the model's predicted outcomes, \n",
    "        marked as 'business_outcome', predicted probability, marked as 'p_hat',\n",
    "        and the inputs in the alphanumerical order of their variables' names or,\n",
    "        should the predicted probability be under that of 75%, a message indicating\n",
    "        as such; if no valid JSON data as far as the application can tell is passed in, \n",
    "        then a JSON encoded error message will be returned instead\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    df -- A DataFrame containing only those 25 column variables required by the pre-trained model\n",
    "    mdl -- A pre-trained, imported prediction model\n",
    "    alphanum_ord_clmn_var_names_lst -- A list of DataFrame column variable names required by \n",
    "                                       the pre-trained model ordered alphanumerically\n",
    "    '''\n",
    "    \n",
    "    num_rows_df = df.shape[0]\n",
    "    \n",
    "    if num_rows_df == 0:\n",
    "        \n",
    "        return {'message': 'ERROR - No valid JSON data available for prediction.'}\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        output_msgs_lst = []\n",
    "    \n",
    "        for row in range(num_rows_df):\n",
    "            predicted_outcome = 0\n",
    "            mdl_inputs = {}\n",
    "\n",
    "            predicted_probability = mdl.predict(df.iloc[row])[0]\n",
    "\n",
    "            if predicted_probability >= 0.75:\n",
    "                predicted_outcome = 1\n",
    "                \n",
    "                for var in alphanum_ord_clmn_var_names_lst:\n",
    "                    mdl_inputs[var] = df.iloc[row][var]\n",
    "\n",
    "                mdl_predictions = {'business_outcome': str(predicted_outcome), \n",
    "                                    'p_hat': str(predicted_probability)}\n",
    "\n",
    "                prediction_msg = mdl_predictions | mdl_inputs\n",
    "\n",
    "                output_msgs_lst.append(prediction_msg)\n",
    "\n",
    "            else:\n",
    "                #output_msgs_lst.append({'message': 'Business outcome probability too low.'})\n",
    "                pass\n",
    "        \n",
    "        return output_msgs_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f476de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rows_per_batch, input_json):\n",
    "    '''\n",
    "    Returns a single event or a list of events as a JSON message containing the business outcome, \n",
    "        probability of said outcome, along with the input variables which led to said outcome \n",
    "        in alphanumerical order for all those predictions which met the minimum standard of 75% \n",
    "        chance of a successful sale to a potential buying customer\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    input_json -- Raw JSON data\n",
    "    '''\n",
    "    json = input_json\n",
    "    \n",
    "    # List of the already properly ordered column variables required by the pre-trained\n",
    "    #    model in order for it to carry out accurate predictions\n",
    "    final_df_column_variable_names_order = [\n",
    "        'x5_saturday', 'x81_July', 'x81_December', 'x31_japan', 'x81_October', \n",
    "        'x5_sunday', 'x31_asia', 'x81_February', 'x91', 'x81_May', \n",
    "        'x5_monday', 'x81_September', 'x81_March', 'x53', 'x81_November', \n",
    "        'x44', 'x81_June', 'x12', 'x5_tuesday', 'x81_August', \n",
    "        'x81_January', 'x62', 'x31_germany', 'x58', 'x56']\n",
    "    \n",
    "    alphanumerically_sorted_df_column_variable_names = sorted(final_df_column_variable_names_order)\n",
    "    \n",
    "    mdl, si, ss = load_models()\n",
    "    \n",
    "    df = transform_json_to_df(json)\n",
    "    \n",
    "    df_batches = batch_df(rows_per_batch, df)\n",
    "    \n",
    "    #batch_details(df_batches)\n",
    "    \n",
    "    transformed_df_batches = extract_transform_df_batches(df_batches, \n",
    "                                                          si, \n",
    "                                                          ss, \n",
    "                                                          final_df_column_variable_names_order)\n",
    "    \n",
    "    print(f'Number Of Bacthes: {len(transformed_df_batches)}')\n",
    "    \n",
    "    batch_details(transformed_df_batches)\n",
    "        \n",
    "    transformed_df = merge_df_batches(transformed_df_batches)\n",
    "    print(f'Merge Batches Total Rows: {len(transformed_df.index)}')\n",
    "    \n",
    "    json_output_message = predict_outcome(transformed_df, \n",
    "                                          mdl, \n",
    "                                          alphanumerically_sorted_df_column_variable_names)\n",
    "        \n",
    "    return json_output_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e57a5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Debugging & Testing Purposes\n",
    "raw_testing_data1 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'))\n",
    "\n",
    "raw_testing_data2 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 1)\n",
    "raw_testing_data3 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 10)\n",
    "raw_testing_data4 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 100)\n",
    "raw_testing_data5 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 1000)\n",
    "\n",
    "#raw_testing_data1.head()\n",
    "#raw_testing_data2.head()\n",
    "#raw_testing_data3.head()\n",
    "#raw_testing_data4.head()\n",
    "#raw_testing_data5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "468a332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_raw_json_10000_rows = raw_testing_data1.to_json(orient = 'records')\n",
    "\n",
    "sample_raw_json_1_row_v2 = raw_testing_data2.to_json(orient = 'records')\n",
    "sample_raw_json_10_rows = raw_testing_data3.to_json(orient = 'records')\n",
    "sample_raw_json_100_rows = raw_testing_data4.to_json(orient = 'records')\n",
    "sample_raw_json_1000_rows = raw_testing_data5.to_json(orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d4f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_raw_json_1_row_v1 = \"{\\\"x0\\\":0.042317,\\\"x1\\\":-3.344721,\\\"x2\\\":4.6351242122,\\\"x3\\\":-0.5983959993,\\\"x4\\\":-0.6477715046,\\\"x5\\\":\\\"monday\\\",\\\"x6\\\":0.184902,\\\"x7\\\":46.690015,\\\"x8\\\":3.034132,\\\"x9\\\":0.364704,\\\"x10\\\":14.260733,\\\"x11\\\":-1.559332,\\\"x12\\\":\\\"$5,547.78\\\",\\\"x13\\\":0.520324,\\\"x14\\\":31.212255,\\\"x15\\\":4.891671,\\\"x16\\\":0.357763,\\\"x17\\\":14.766366,\\\"x18\\\":-17.467243,\\\"x19\\\":0.224628,\\\"x20\\\":0.096752,\\\"x21\\\":1.305564,\\\"x22\\\":0.353632,\\\"x23\\\":3.909028,\\\"x24\\\":-91.273052,\\\"x25\\\":1.396952,\\\"x26\\\":4.401593,\\\"x27\\\":0.443086,\\\"x28\\\":14.048787,\\\"x29\\\":-0.932243,\\\"x30\\\":5.255472,\\\"x31\\\":\\\"germany\\\",\\\"x32\\\":0.54199153,\\\"x33\\\":2.98948039,\\\"x34\\\":-1.78334189,\\\"x35\\\":0.80127315,\\\"x36\\\":-2.60231221,\\\"x37\\\":3.39682926,\\\"x38\\\":-1.22322646,\\\"x39\\\":-2.20977636,\\\"x40\\\":-68.69,\\\"x41\\\":522.25,\\\"x42\\\":-428.69,\\\"x43\\\":381.37,\\\"x44\\\":0.0197503,\\\"x45\\\":0.75116479,\\\"x46\\\":0.8630479008,\\\"x47\\\":-1.0383166613,\\\"x48\\\":-0.2726187635,\\\"x49\\\":-0.3430207259,\\\"x50\\\":0.3109008666,\\\"x51\\\":-0.797841974,\\\"x52\\\":-2.0390175153,\\\"x53\\\":0.87182889,\\\"x54\\\":0.14373012,\\\"x55\\\":-1.15212514,\\\"x56\\\":-2.1703139704,\\\"x57\\\":-0.267842962,\\\"x58\\\":0.212110633,\\\"x59\\\":1.6926559407,\\\"x60\\\":-0.9522767913,\\\"x61\\\":-0.8625864974,\\\"x62\\\":0.0748487158,\\\"x63\\\":\\\"36.29%\\\",\\\"x64\\\":3.47125327,\\\"x65\\\":-3.16656509,\\\"x66\\\":0.65446814,\\\"x67\\\":14.60067029,\\\"x68\\\":-20.57521013,\\\"x69\\\":0.71083785,\\\"x70\\\":0.16983767,\\\"x71\\\":0.55082127,\\\"x72\\\":0.62814576,\\\"x73\\\":3.38608078,\\\"x74\\\":-112.45263714,\\\"x75\\\":1.48370808,\\\"x76\\\":1.77035368,\\\"x77\\\":0.75702363,\\\"x78\\\":14.75731742,\\\"x79\\\":-0.62550355,\\\"x80\\\":null,\\\"x81\\\":\\\"October\\\",\\\"x82\\\":\\\"Female\\\",\\\"x83\\\":-0.7116680715,\\\"x84\\\":-0.2653559892,\\\"x85\\\":0.5175495907,\\\"x86\\\":-1.0881027092,\\\"x87\\\":-1.8188638198,\\\"x88\\\":-1.3584469527,\\\"x89\\\":-0.654995195,\\\"x90\\\":-0.4933042262,\\\"x91\\\":0.373853,\\\"x92\\\":0.94143481,\\\"x93\\\":3.54679834,\\\"x94\\\":-99.8574882,\\\"x95\\\":0.403926,\\\"x96\\\":1.65378726,\\\"x97\\\":0.00771459,\\\"x98\\\":-32.02164582,\\\"x99\\\":-60.3127828}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "995b342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves raw JSON data files for proper testing when Docker becomes involved\n",
    "#with open(os.path.join('../testing', 'sample_raw_json_1_row_v1.json'), 'w') as file:\n",
    "#    file.write(sample_raw_json_1_row_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4d58dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves slices of raw testing data of various sizes whilst \n",
    "#    converting and saving them into a JSON format\n",
    "#sample_raw_json_10000_rows = raw_testing_data1.to_json(os.path.join('../testing', 'sample_raw_json_10000_rows.json'), orient = 'records')\n",
    "\n",
    "#sample_raw_json_1_row_v2 = raw_testing_data2.to_json(os.path.join('../testing', 'sample_raw_json_1_row_v2.json'), orient = 'records')\n",
    "#sample_raw_json_10_rows = raw_testing_data3.to_json(os.path.join('../testing', 'sample_raw_json_10_rows.json'), orient = 'records')\n",
    "#sample_raw_json_100_rows = raw_testing_data4.to_json(os.path.join('../testing', 'sample_raw_json_100_rows.json'), orient = 'records')\n",
    "#sample_raw_json_1000_rows = raw_testing_data5.to_json(os.path.join('../testing', 'sample_raw_json_1000_rows.json'), orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0d5aba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.065676</td>\n",
       "      <td>1.892277</td>\n",
       "      <td>4.818741</td>\n",
       "      <td>0.640313</td>\n",
       "      <td>1.944562</td>\n",
       "      <td>friday</td>\n",
       "      <td>0.208718</td>\n",
       "      <td>73.573314</td>\n",
       "      <td>4.929132</td>\n",
       "      <td>0.116004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305243</td>\n",
       "      <td>-0.099213</td>\n",
       "      <td>0.712234</td>\n",
       "      <td>3.853489</td>\n",
       "      <td>-91.650053</td>\n",
       "      <td>0.499861</td>\n",
       "      <td>2.804358</td>\n",
       "      <td>0.627921</td>\n",
       "      <td>-32.190043</td>\n",
       "      <td>103.192597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.357126</td>\n",
       "      <td>-1.852161</td>\n",
       "      <td>5.367849</td>\n",
       "      <td>-0.069869</td>\n",
       "      <td>-0.641455</td>\n",
       "      <td>saturday</td>\n",
       "      <td>0.940286</td>\n",
       "      <td>72.773335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.191044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617258</td>\n",
       "      <td>0.307445</td>\n",
       "      <td>0.376738</td>\n",
       "      <td>3.306958</td>\n",
       "      <td>-99.557140</td>\n",
       "      <td>1.275527</td>\n",
       "      <td>1.476482</td>\n",
       "      <td>0.122798</td>\n",
       "      <td>-32.957087</td>\n",
       "      <td>-111.509168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.142132</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>3.917673</td>\n",
       "      <td>-0.722627</td>\n",
       "      <td>0.412905</td>\n",
       "      <td>friday</td>\n",
       "      <td>0.975457</td>\n",
       "      <td>86.782049</td>\n",
       "      <td>2.480363</td>\n",
       "      <td>0.229048</td>\n",
       "      <td>...</td>\n",
       "      <td>1.023861</td>\n",
       "      <td>0.507509</td>\n",
       "      <td>0.485926</td>\n",
       "      <td>3.852836</td>\n",
       "      <td>-94.374115</td>\n",
       "      <td>1.193276</td>\n",
       "      <td>0.700849</td>\n",
       "      <td>0.885308</td>\n",
       "      <td>-32.111276</td>\n",
       "      <td>61.841748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2        x3        x4        x5        x6  \\\n",
       "3 -0.065676  1.892277  4.818741  0.640313  1.944562    friday  0.208718   \n",
       "4 -0.357126 -1.852161  5.367849 -0.069869 -0.641455  saturday  0.940286   \n",
       "5 -1.142132  0.012474  3.917673 -0.722627  0.412905    friday  0.975457   \n",
       "\n",
       "          x7        x8        x9  ...       x90       x91       x92       x93  \\\n",
       "3  73.573314  4.929132  0.116004  ...  0.305243 -0.099213  0.712234  3.853489   \n",
       "4  72.773335       NaN  0.191044  ...  0.617258  0.307445  0.376738  3.306958   \n",
       "5  86.782049  2.480363  0.229048  ...  1.023861  0.507509  0.485926  3.852836   \n",
       "\n",
       "         x94       x95       x96       x97        x98         x99  \n",
       "3 -91.650053  0.499861  2.804358  0.627921 -32.190043  103.192597  \n",
       "4 -99.557140  1.275527  1.476482  0.122798 -32.957087 -111.509168  \n",
       "5 -94.374115  1.193276  0.700849  0.885308 -32.111276   61.841748  \n",
       "\n",
       "[3 rows x 100 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_testing_data3.iloc[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef724661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_details(df_batches):\n",
    "    row_cnt = 0\n",
    "    \n",
    "    for index in range(len(df_batches)):\n",
    "        \n",
    "        print(f'Rows in Batch #{index}: {len((df_batches[index]).index)}')\n",
    "        \n",
    "        row_cnt += len((df_batches[index]).index)\n",
    "    \n",
    "    print(f'Total Rows: {row_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25f5145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_details(json_res, print_res):\n",
    "    print(f'Results Type: {type(json_res)}\\n')\n",
    "    print(f'Results Length: {len(json_res)}\\n')\n",
    "    \n",
    "    if isinstance(json_res, list) and print_res == True:\n",
    "        for result in json_res:\n",
    "            print(f'{result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a5c648a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Bacthes: 1\n",
      "Rows in Batch #0: 1\n",
      "Total Rows: 1\n",
      "Merge Batches Total Rows: 1\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 0\n",
    "json_results1 = main(500, sample_raw_json_1_row_v1)\n",
    "results_details(json_results1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "080cfd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Bacthes: 1\n",
      "Rows in Batch #0: 1\n",
      "Total Rows: 1\n",
      "Merge Batches Total Rows: 1\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 0\n",
    "json_results2 = main(500, sample_raw_json_1_row_v2)\n",
    "results_details(json_results2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fe4e7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Bacthes: 1\n",
      "Rows in Batch #0: 10\n",
      "Total Rows: 10\n",
      "Merge Batches Total Rows: 10\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 2\n",
      "\n",
      "{'business_outcome': '1', 'p_hat': '0.8227526744741207', 'x12': -0.9529553121992512, 'x31_asia': 0.0, 'x31_germany': 0.0, 'x31_japan': 0.0, 'x44': -0.018906143399875407, 'x53': -1.5974242627501094, 'x56': 1.6483959900371876, 'x58': 0.599511719639024, 'x5_monday': 0.0, 'x5_saturday': 0.0, 'x5_sunday': 0.0, 'x5_tuesday': 1.0, 'x62': 0.7274434380391581, 'x81_August': 0.0, 'x81_December': 0.0, 'x81_February': 0.0, 'x81_January': 0.0, 'x81_July': 0.0, 'x81_June': 0.0, 'x81_March': 0.0, 'x81_May': 0.0, 'x81_November': 1.0, 'x81_October': 0.0, 'x81_September': 0.0, 'x91': -0.3633580994460479}\n",
      "\n",
      "{'business_outcome': '1', 'p_hat': '0.7539305190612656', 'x12': 1.1180442609776802, 'x31_asia': 0.0, 'x31_germany': 1.0, 'x31_japan': 0.0, 'x44': 1.0491037054747567, 'x53': -1.0452443300416803, 'x56': -0.9224261424370558, 'x58': 0.41258044127697047, 'x5_monday': 0.0, 'x5_saturday': 0.0, 'x5_sunday': 0.0, 'x5_tuesday': 0.0, 'x62': -0.29478102388959054, 'x81_August': 0.0, 'x81_December': 0.0, 'x81_February': 0.0, 'x81_January': 0.0, 'x81_July': 0.0, 'x81_June': 0.0, 'x81_March': 0.0, 'x81_May': 0.0, 'x81_November': 0.0, 'x81_October': 0.0, 'x81_September': 1.0, 'x91': 1.2247713571905237}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 2\n",
    "json_results3 = main(500, sample_raw_json_10_rows)\n",
    "results_details(json_results3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6245668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Bacthes: 1\n",
      "Rows in Batch #0: 100\n",
      "Total Rows: 100\n",
      "Merge Batches Total Rows: 100\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 24\n",
    "json_results4 = main(500, sample_raw_json_100_rows)\n",
    "results_details(json_results4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfde8139",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Bacthes: 2\n",
      "Rows in Batch #0: 500\n",
      "Rows in Batch #1: 500\n",
      "Total Rows: 1000\n",
      "Merge Batches Total Rows: 1000\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 215\n",
    "json_results5 = main(500, sample_raw_json_1000_rows)\n",
    "results_details(json_results5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1b34fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Bacthes: 20\n",
      "Rows in Batch #0: 500\n",
      "Rows in Batch #1: 500\n",
      "Rows in Batch #2: 500\n",
      "Rows in Batch #3: 500\n",
      "Rows in Batch #4: 500\n",
      "Rows in Batch #5: 500\n",
      "Rows in Batch #6: 500\n",
      "Rows in Batch #7: 500\n",
      "Rows in Batch #8: 500\n",
      "Rows in Batch #9: 500\n",
      "Rows in Batch #10: 500\n",
      "Rows in Batch #11: 500\n",
      "Rows in Batch #12: 500\n",
      "Rows in Batch #13: 500\n",
      "Rows in Batch #14: 500\n",
      "Rows in Batch #15: 500\n",
      "Rows in Batch #16: 500\n",
      "Rows in Batch #17: 500\n",
      "Rows in Batch #18: 500\n",
      "Rows in Batch #19: 500\n",
      "Total Rows: 10000\n",
      "Merge Batches Total Rows: 10000\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 2013\n",
    "json_results6 = main(500, sample_raw_json_10000_rows)\n",
    "results_details(json_results6, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f69e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
