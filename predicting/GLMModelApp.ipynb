{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a35dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Required Libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "import uvicorn\n",
    "\n",
    "from fastapi import FastAPI\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e87b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    '''\n",
    "    Returns loaded in, pre-trained Prediction, SimpleImputer and StandardScalar models\n",
    "    '''\n",
    "    \n",
    "    mdl = sm.load('../models/glm_final_model.pickle')\n",
    "    si = pickle.load(open('../models/glm_simple_imputer.pickle', 'rb'))\n",
    "    ss = pickle.load(open('../models/glm_standard_scalar.pickle', 'rb'))\n",
    "    \n",
    "    return mdl, si, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b6360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_json_to_df(json):\n",
    "    '''\n",
    "    Returns a DataFrame containing raw JSON data\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    json -- Raw JSON data\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_json(json, orient = 'records')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411ecffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_column_variables(df):\n",
    "    '''\n",
    "    Returns a DataFrame after having transformed those columns, whose values \n",
    "        consisted of those of data types String and represented quantitative \n",
    "        data, into values being that of the Float data type after removing any\n",
    "        non mathematical symbols \n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- A DataFrame containing raw JSON data\n",
    "    '''\n",
    "    \n",
    "    # Formats the 'x12' and 'x63' columns', consisting of the data type String, \n",
    "    #    and respectively representing first monetary then percentage values,\n",
    "    #    into the data type Float so as to be able to latter on apply mathermatical\n",
    "    #    work upon said columns' values later on\n",
    "\n",
    "    df['x12'] = df['x12'].str.replace('$', '')\n",
    "    df['x12'] = df['x12'].str.replace(',', '')\n",
    "    df['x12'] = df['x12'].str.replace(')', '')\n",
    "    df['x12'] = df['x12'].str.replace('(', '-')\n",
    "    df['x12'] = df['x12'].astype(float)\n",
    "    \n",
    "    df['x63'] = df['x63'].str.replace('%', '')\n",
    "    df['x63'] = df['x63'].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc361ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_df_data(si, df):\n",
    "    '''\n",
    "    Returns a DataFrame with no column variables whose data is of a qualitative nature, \n",
    "        as well as having filled in any remaining blank, NaN or NULL or otherwise missing \n",
    "        values via the usage of an imported, pre-trained SimpleImputer using a mean based \n",
    "        strategy\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    df -- A formatted DataFrame\n",
    "    '''\n",
    "    \n",
    "    #if df.shape[0] <= 1:\n",
    "    #    df = pd.DataFrame(df.drop(columns = ['x5', 'x31', 'x81', 'x82']), \n",
    "    #                      columns = df.drop(columns = ['x5', 'x31', 'x81', 'x82']).columns)\n",
    "    #    df = df.fillna(0)\n",
    "        \n",
    "    #else:\n",
    "    df = pd.DataFrame(si.transform(df.drop(columns = ['x5', 'x31', 'x81', 'x82'])), \n",
    "                      columns = df.drop(columns = ['x5', 'x31', 'x81', 'x82']).columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3211268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df_data(ss, df):\n",
    "    '''\n",
    "    Returns a DataFrame whose column variable values have all been scaled via a \n",
    "        standardization method for the purpose of feature scaling, utilizing an\n",
    "        imported. pre-trained StandardScalar\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    df -- A formatted DataFrame without any blank, NaN, NULL or otherwise missing values\n",
    "    '''\n",
    "    \n",
    "    # Of particular interest and focus is that of the 'x12' column representing\n",
    "    #    monetary values which tend to outscale all other column variable values\n",
    "    #    by some orders of magnitude\n",
    "    \n",
    "    df = pd.DataFrame(ss.transform(df), \n",
    "                      columns = df.columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65d9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_dummy_column_variables_new(df1, df2):\n",
    "    '''\n",
    "    Returns a DataFrame with dummy variables for those column variables consisting of\n",
    "        qualitative data, as well as whose column variables of numeric quantitative data\n",
    "        have no missing values and are scaled\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    df1 -- A DataFrame containing the original raw JSON data in order to retrieve those column\n",
    "           variable values of a qualitative nature previously dropped and must now be \n",
    "           dummified\n",
    "    df2 -- A DataFrame with no missing column variable values and whose said values have already been scaled\n",
    "    '''\n",
    "    \n",
    "    # A list of column variable names of a quantitative nature which require dummification\n",
    "    vars_to_dummify = ['x5', 'x31', 'x81', 'x82']\n",
    "    \n",
    "    for var in vars_to_dummify:\n",
    "\n",
    "        var_dummy_vars = pd.get_dummies(df1[var], \n",
    "                                        drop_first = True, \n",
    "                                        prefix = var, \n",
    "                                        prefix_sep = '_', \n",
    "                                        dummy_na = True)\n",
    "\n",
    "\n",
    "        df2 = pd.concat([df2, var_dummy_vars], \n",
    "                        axis = 1, \n",
    "                        sort = False)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca533eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_dummy_column_variables_old(df1, df2):\n",
    "    '''\n",
    "    Returns a DataFrame with dummy variables for those column variables consisting of\n",
    "        qualitative data, as well as whose column variables of numeric quantitative data\n",
    "        have no missing values and are scaled\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    df1 -- A DataFrame containing the original raw JSON data in order to retrieve those column\n",
    "           variable values of a qualitative nature previously dropped and must now be \n",
    "           dummified\n",
    "    df2 -- A DataFrame with no missing column variable values and whose said values have already been scaled\n",
    "    '''\n",
    "    \n",
    "    x5_dummy_variables = pd.get_dummies(df1['x5'], \n",
    "                                    drop_first = True, \n",
    "                                    prefix = 'x5', \n",
    "                                    prefix_sep = '_', \n",
    "                                    dummy_na = True)\n",
    "\n",
    "    df2 = pd.concat([df2, x5_dummy_variables], axis = 1, sort = False)\n",
    "\n",
    "    x31_dummy_variables = pd.get_dummies(df1['x31'], \n",
    "                                         drop_first = True, \n",
    "                                         prefix = 'x31', \n",
    "                                         prefix_sep = '_', \n",
    "                                         dummy_na = True)\n",
    "    \n",
    "    df2 = pd.concat([df2, x31_dummy_variables], axis = 1, sort = False)\n",
    "\n",
    "    x81_dummy_variables = pd.get_dummies(df1['x81'], \n",
    "                                         drop_first = True, \n",
    "                                         prefix = 'x81', \n",
    "                                         prefix_sep = '_', \n",
    "                                         dummy_na = True)\n",
    "\n",
    "    df2 = pd.concat([df2, x81_dummy_variables], axis = 1, sort = False)\n",
    "\n",
    "    x82_dummy_variables = pd.get_dummies(df1['x82'], \n",
    "                                         drop_first = True, \n",
    "                                         prefix = 'x82', \n",
    "                                         prefix_sep = '_', \n",
    "                                         dummy_na = True)\n",
    "\n",
    "    df2 = pd.concat([df2, x82_dummy_variables], axis = 1, sort = False)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8f337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_column_variables(ordr_clmn_names_lst, df):\n",
    "    '''\n",
    "    Returns a DataFrame containing only those column variables required by the pre-trained model\n",
    "        for predictions, filtering out drom the given DataFrame only said columns\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    ordr_clmn_names_lst -- A list of DataFrame column variable names required by the pre-trained model\n",
    "    df -- The DataFrame with dummy variables, and whose quantitative values have been scaled and \n",
    "          have none missing\n",
    "    '''\n",
    "    \n",
    "    necessary_clmn_vars_set = set(ordr_clmn_names_lst)\n",
    "    avlbl_clmn_vars_set = set(df.columns)\n",
    "\n",
    "    # Depending upon the type and amount of JSON data originally having been passed in, \n",
    "    #    not all of the desired dummy variables will always be successfully generated, \n",
    "    #    necessitating their inclusion afterwards via the code below\n",
    "    if necessary_clmn_vars_set.issubset(avlbl_clmn_vars_set) == False:\n",
    "        nan_df = pd.DataFrame(np.nan, index = range(df.shape[0]), columns = ordr_clmn_names_lst)\n",
    "        df = df.combine_first(nan_df)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "    df = df[ordr_clmn_names_lst].copy(deep = True)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c64c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transform_input_data_pipeline(json, si, ss, ordr_clmn_names_lst):\n",
    "    '''\n",
    "    Returns a DataFrame created from the passed in, raw JSON data, transforming said data via\n",
    "        imputation, to fill in any and all missing values, scaling, and the creation of dummy\n",
    "        variables for those qualitative column variables which require such action\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    json -- Raw JSON data\n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    ordr_clmn_names_lst -- A list of DataFrame column variable names required by the pre-trained model\n",
    "    '''\n",
    "    \n",
    "    df = transform_json_to_df(json)\n",
    "    \n",
    "    if df.shape[0] < 1:\n",
    "        df = pd.DataFrame()\n",
    " \n",
    "    else:\n",
    "        df = format_df_column_variables(df)\n",
    "        #print('Format')\n",
    "        #print(df.head())\n",
    "        #print(df.columns)\n",
    "\n",
    "        imputed_df = impute_missing_df_data(si, df)\n",
    "        #print('Impute')\n",
    "        #print(imputed_df.head())\n",
    "        #print(imputed_df.columns)\n",
    "\n",
    "        scaled_imputed_df = scale_df_data(ss, imputed_df)\n",
    "        #print('Scale')\n",
    "        #print(scaled_imputed_df.head())\n",
    "        #print(scaled_imputed_df.columns)\n",
    "\n",
    "        df = create_df_dummy_column_variables_old(df, scaled_imputed_df)\n",
    "        #print(df.head())\n",
    "        #print(df.columns)\n",
    "\n",
    "        df = filter_df_column_variables(ordr_clmn_names_lst, df)\n",
    "        #print('Filter')\n",
    "        #print(df.head())\n",
    "        #print(df.columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38021796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_outcome(df, mdl, alphanum_ord_clmn_var_names_lst):\n",
    "    '''\n",
    "    Returns a JSON message containing either the model's predicted outcomes, \n",
    "        marked as 'business_outcome', predicted probability, marked as 'p_hat',\n",
    "        and the inputs in the alphanumerical order of their variables' names or,\n",
    "        should the predicted probability be under that of 75%, a message indicating\n",
    "        as such; if no valid JSON data as far as the application can tell is passed in, \n",
    "        then a JSON encoded error message will be returned instead\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    df -- A DataFrame containing only those 25 column variables required by the pre-trained model\n",
    "    mdl -- A pre-trained, imported prediction model\n",
    "    alphanum_ord_clmn_var_names_lst -- A list of DataFrame column variable names required by \n",
    "                                       the pre-trained model order alphanumerically\n",
    "    '''\n",
    "    \n",
    "    num_rows_df = df.shape[0]\n",
    "    \n",
    "    if num_rows_df == 0:\n",
    "        \n",
    "        return json.dumps({'message': 'ERROR - No valid JSON data available for prediction.'})\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        output_msgs_lst = []\n",
    "        count_output_msgs = 0\n",
    "    \n",
    "        for row in range(num_rows_df):\n",
    "            predicted_outcome = 0\n",
    "            mdl_inputs = {}\n",
    "\n",
    "            predicted_probability = mdl.predict(df.iloc[row])[0]\n",
    "\n",
    "            if predicted_probability >= 0.75:\n",
    "                predicted_outcome = 1\n",
    "                \n",
    "            for var in alphanum_ord_clmn_var_names_lst:\n",
    "                mdl_inputs[var] = df.iloc[row][var]\n",
    "\n",
    "            mdl_predictions = {'business_outcome': str(predicted_outcome), \n",
    "                               'p_hat': str(predicted_probability)}\n",
    "\n",
    "            prediction_msg = mdl_predictions | mdl_inputs\n",
    "                \n",
    "            output_msgs_lst.append(prediction_msg)\n",
    "\n",
    "            #else:\n",
    "                #output_msgs_lst.append(\"{'message': 'Business outcome probability too low.'}\")\n",
    "                #pass\n",
    "            \n",
    "            #count_output_msgs += 1\n",
    "            #print(count_output_msgs)\n",
    "        \n",
    "        return json.dumps(output_msgs_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f476de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_json):\n",
    "    \n",
    "    json = input_json\n",
    "    \n",
    "    # List of the already properly ordered column variables required by the pre-trained\n",
    "    #    model in order for it to carry out accurate predictions\n",
    "    final_df_column_variable_names_order = [\n",
    "        'x5_saturday', 'x81_July', 'x81_December', 'x31_japan', 'x81_October', \n",
    "        'x5_sunday', 'x31_asia', 'x81_February', 'x91', 'x81_May', \n",
    "        'x5_monday', 'x81_September', 'x81_March', 'x53', 'x81_November', \n",
    "        'x44', 'x81_June', 'x12', 'x5_tuesday', 'x81_August', \n",
    "        'x81_January', 'x62', 'x31_germany', 'x58', 'x56']\n",
    "    \n",
    "    alphanumerically_sorted_df_column_variable_names = sorted(final_df_column_variable_names_order)\n",
    "    \n",
    "    mdl, si, ss = load_models()\n",
    "    \n",
    "    df = extract_transform_input_data_pipeline(json, \n",
    "                                               si, \n",
    "                                               ss, \n",
    "                                               final_df_column_variable_names_order)\n",
    "    \n",
    "    json_output_message = predict_outcome(df, mdl, alphanumerically_sorted_df_column_variable_names)\n",
    "    \n",
    "    json_msgs_results = json_output_message.split(\"}, \")\n",
    "    print(f'Number of Predictions: {len(json_msgs_results)} \\n')\n",
    "    \n",
    "    #print(json_output_message[:1000])\n",
    "    \n",
    "    for json_msg in json_msgs_results:\n",
    "        print(json_msg[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e57a5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_testing_data1 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'))\n",
    "raw_testing_data2 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 1)\n",
    "raw_testing_data3 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 10)\n",
    "raw_testing_data4 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 100)\n",
    "raw_testing_data5 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 1000)\n",
    "\n",
    "sample_json1 = raw_testing_data1.to_json(orient = 'records')\n",
    "sample_json2 = raw_testing_data2.to_json(orient = 'records')\n",
    "sample_json3 = raw_testing_data3.to_json(orient = 'records')\n",
    "sample_json4 = raw_testing_data4.to_json(orient = 'records')\n",
    "sample_json5 = raw_testing_data5.to_json(orient = 'records')\n",
    "\n",
    "#raw_testing_data1.head()\n",
    "#raw_testing_data2.head()\n",
    "#raw_testing_data3.head()\n",
    "#raw_testing_data4.head()\n",
    "#raw_testing_data5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb11999",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_json_string = \"{\\\"x0\\\":0.042317,\\\"x1\\\":-3.344721,\\\"x2\\\":4.6351242122,\\\"x3\\\":-0.5983959993,\\\"x4\\\":-0.6477715046,\\\"x5\\\":\\\"monday\\\",\\\"x6\\\":0.184902,\\\"x7\\\":46.690015,\\\"x8\\\":3.034132,\\\"x9\\\":0.364704,\\\"x10\\\":14.260733,\\\"x11\\\":-1.559332,\\\"x12\\\":\\\"$5,547.78\\\",\\\"x13\\\":0.520324,\\\"x14\\\":31.212255,\\\"x15\\\":4.891671,\\\"x16\\\":0.357763,\\\"x17\\\":14.766366,\\\"x18\\\":-17.467243,\\\"x19\\\":0.224628,\\\"x20\\\":0.096752,\\\"x21\\\":1.305564,\\\"x22\\\":0.353632,\\\"x23\\\":3.909028,\\\"x24\\\":-91.273052,\\\"x25\\\":1.396952,\\\"x26\\\":4.401593,\\\"x27\\\":0.443086,\\\"x28\\\":14.048787,\\\"x29\\\":-0.932243,\\\"x30\\\":5.255472,\\\"x31\\\":\\\"germany\\\",\\\"x32\\\":0.54199153,\\\"x33\\\":2.98948039,\\\"x34\\\":-1.78334189,\\\"x35\\\":0.80127315,\\\"x36\\\":-2.60231221,\\\"x37\\\":3.39682926,\\\"x38\\\":-1.22322646,\\\"x39\\\":-2.20977636,\\\"x40\\\":-68.69,\\\"x41\\\":522.25,\\\"x42\\\":-428.69,\\\"x43\\\":381.37,\\\"x44\\\":0.0197503,\\\"x45\\\":0.75116479,\\\"x46\\\":0.8630479008,\\\"x47\\\":-1.0383166613,\\\"x48\\\":-0.2726187635,\\\"x49\\\":-0.3430207259,\\\"x50\\\":0.3109008666,\\\"x51\\\":-0.797841974,\\\"x52\\\":-2.0390175153,\\\"x53\\\":0.87182889,\\\"x54\\\":0.14373012,\\\"x55\\\":-1.15212514,\\\"x56\\\":-2.1703139704,\\\"x57\\\":-0.267842962,\\\"x58\\\":0.212110633,\\\"x59\\\":1.6926559407,\\\"x60\\\":-0.9522767913,\\\"x61\\\":-0.8625864974,\\\"x62\\\":0.0748487158,\\\"x63\\\":\\\"36.29%\\\",\\\"x64\\\":3.47125327,\\\"x65\\\":-3.16656509,\\\"x66\\\":0.65446814,\\\"x67\\\":14.60067029,\\\"x68\\\":-20.57521013,\\\"x69\\\":0.71083785,\\\"x70\\\":0.16983767,\\\"x71\\\":0.55082127,\\\"x72\\\":0.62814576,\\\"x73\\\":3.38608078,\\\"x74\\\":-112.45263714,\\\"x75\\\":1.48370808,\\\"x76\\\":1.77035368,\\\"x77\\\":0.75702363,\\\"x78\\\":14.75731742,\\\"x79\\\":-0.62550355,\\\"x80\\\":null,\\\"x81\\\":\\\"October\\\",\\\"x82\\\":\\\"Female\\\",\\\"x83\\\":-0.7116680715,\\\"x84\\\":-0.2653559892,\\\"x85\\\":0.5175495907,\\\"x86\\\":-1.0881027092,\\\"x87\\\":-1.8188638198,\\\"x88\\\":-1.3584469527,\\\"x89\\\":-0.654995195,\\\"x90\\\":-0.4933042262,\\\"x91\\\":0.373853,\\\"x92\\\":0.94143481,\\\"x93\\\":3.54679834,\\\"x94\\\":-99.8574882,\\\"x95\\\":0.403926,\\\"x96\\\":1.65378726,\\\"x97\\\":0.00771459,\\\"x98\\\":-32.02164582,\\\"x99\\\":-60.3127828}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ca932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(os.path.join('../testing', 'sample_json1.json'), 'w') as file:\n",
    "#    json.dump(sample_json1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a5c648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Predictions: 10 \n",
      "\n",
      "[{\"business_outcome\": \"0\", \"p_hat\": \"0.3643270499231786\", \"x12\": 0.9570362947692982, \"x31_asia\": 0.0, \"x31_germany\": 1.0, \"x31_japan\": 0.0, \"x44\": -1.6951577772991397, \"x53\": 1.646164213360519, \"x56\": -2.1739025236914333, \"x58\": 0.2134890514411317, \"x5_monday\": 1.0, \"x5_saturday\": 0.0, \"x5_sunday\": 0.0, \"x5_tuesday\": 0.0, \"x62\": 0.06850025352534539, \"x81_August\": 0.0, \"x81_December\": 0.0, \"x81_February\": 0.0, \"x81_January\": 0.0, \"x81_July\": 0.0, \"x81_June\": 0.0, \"x81_March\": 0.0, \"x81_May\": 0.0, \"x81_November\": 0.0, \"x81_October\": 1.0, \"x81_September\": 0.0, \"x91\": 0.42018884695505015}, {\"business_outcome\": \"1\", \"p_hat\": \"0.8227526744741207\", \"x12\": -0.9529553121992512, \"x31_asia\": 0.0, \"x31_germany\": 0.0, \"x31_japan\": 0.0, \"x44\": -0.018906143399875407, \"x53\": -1.5974242627501094, \"x56\": 1.6483959900371876, \"x58\": 0.599511719639024, \"x5_monday\": 0.0, \"x5_saturday\": 0.0, \"x5_sunday\": 0.0, \"x5_tuesday\": 1.0, \"x62\": 0.7274434380391581, \"x81_August\": 0.0, \"x81_December\": 0.0, \"x81_February\"\n",
      "[{\"business_outcome\": \"0\", \"p_hat\": \"0.36432704992\n",
      "{\"business_outcome\": \"1\", \"p_hat\": \"0.822752674474\n",
      "{\"business_outcome\": \"0\", \"p_hat\": \"0.134106250645\n",
      "{\"business_outcome\": \"0\", \"p_hat\": \"0.469681103329\n",
      "{\"business_outcome\": \"0\", \"p_hat\": \"0.322071394732\n",
      "{\"business_outcome\": \"0\", \"p_hat\": \"0.551230100593\n",
      "{\"business_outcome\": \"0\", \"p_hat\": \"0.496666346938\n",
      "{\"business_outcome\": \"1\", \"p_hat\": \"0.753930519061\n",
      "{\"business_outcome\": \"0\", \"p_hat\": \"0.605559236151\n",
      "{\"business_outcome\": \"0\", \"p_hat\": \"0.177572116678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/dtjmhvc55bd1swsb_wk1cnbr0000gn/T/ipykernel_52486/1016609141.py:17: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['x12'] = df['x12'].str.replace('$', '')\n",
      "/var/folders/tc/dtjmhvc55bd1swsb_wk1cnbr0000gn/T/ipykernel_52486/1016609141.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['x12'] = df['x12'].str.replace(')', '')\n",
      "/var/folders/tc/dtjmhvc55bd1swsb_wk1cnbr0000gn/T/ipykernel_52486/1016609141.py:20: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['x12'] = df['x12'].str.replace('(', '-')\n"
     ]
    }
   ],
   "source": [
    "main(sample_json3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802254de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
