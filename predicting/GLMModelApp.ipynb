{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a9c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import uvicorn\n",
    "\n",
    "from fastapi import FastAPI\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcbda8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and instantiates the FastAPI application object\n",
    "app = FastAPI(title = \"GLM Model Application\",\n",
    "              description = \"API Application for GLM Model\", \n",
    "              version = \"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7076939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the index route, which opens automatically upon http://127.0.0.1:1313\n",
    "@app.get('/')\n",
    "def index():\n",
    "    return json.dumps({'message': 'Successfully connected to GLM Model Application.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db76000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carries out a prediction upon passed in data\n",
    "@app.post('/predict')\n",
    "def predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91126a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts up an instance of a uvicorn server in order for the FastAPI application object to run upon\n",
    "#if __name__ == '__main__':\n",
    "#    uvicorn.run(app, host = '127.0.0.1', port = 1313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe60ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \n",
    "    mdl = sm.load('glm_final_model.pickle')\n",
    "    \n",
    "    return mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef1e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_json_to_df(json):\n",
    "    \n",
    "    df = pd.read_json(json, orient = 'records')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e04c8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_df_rows_missing_data(df):\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "470dd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_column_variables(df):\n",
    "    \n",
    "    # Formats the 'x12' column's String monetary value into a Float in order to apply maths upon it\n",
    "    df['x12'] = df['x12'].str.replace('$', '')\n",
    "    df['x12'] = df['x12'].str.replace(',', '')\n",
    "    df['x12'] = df['x12'].str.replace(')', '')\n",
    "    df['x12'] = df['x12'].str.replace('(', '-')\n",
    "    df['x12'] = df['x12'].astype(float)\n",
    "    \n",
    "    df['x63'] = df['x63'].str.replace('%', '')\n",
    "    df['x63'] = df['x63'].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb9ed01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_df_data(df):\n",
    "    \n",
    "    # Creates and instantiates a simple imputer\n",
    "    si = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "    \n",
    "    # Imputes via a simple mean stategy those column values which are missing\n",
    "    df = pd.DataFrame(si.fit_transform(df.drop(columns = ['x5', 'x31', 'x81', 'x82'])), \n",
    "                      columns = df.drop(columns = ['x5', 'x31', 'x81', 'x82']).columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1de4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df_data(df):\n",
    "    \n",
    "    # Creates and instantiates a standard scaler\n",
    "    ss = StandardScaler()\n",
    "    \n",
    "    '''\n",
    "    Scales all column values via a standardization method for feature scaling, \n",
    "        of particular interest and focus being being that of the monetary value column\n",
    "    '''\n",
    "    df = pd.DataFrame(ss.fit_transform(df), \n",
    "                      columns = df.columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edfb8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_dummy_column_variables_new(df1, df2):\n",
    "    \n",
    "    '''\n",
    "    Creates the dummy variables for the non-numeric, qualitative data type columns and then reconcatenates\n",
    "        them back into the now imputed and standardized scaled dataframe\n",
    "    '''\n",
    "    vars_to_dummify = ['x5', 'x31', 'x81', 'x82']\n",
    "    \n",
    "    for var in vars_to_dummify:\n",
    "\n",
    "        var_dummy_vars = pd.get_dummies(df1[var], \n",
    "                                        drop_first = True, \n",
    "                                        prefix = var, \n",
    "                                        prefix_sep = '_', \n",
    "                                        dummy_na = True)\n",
    "\n",
    "\n",
    "        df2 = pd.concat([df2, var_dummy_vars], \n",
    "                        axis = 1, \n",
    "                        sort = False)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d4c14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_dummy_column_variables_old(df1, df2):\n",
    "    \n",
    "    x5_dummy_variables = pd.get_dummies(df1['x5'], \n",
    "                                    drop_first = True, \n",
    "                                    prefix = 'x5', \n",
    "                                    prefix_sep = '_', \n",
    "                                    dummy_na = True)\n",
    "\n",
    "    df2 = pd.concat([df2, x5_dummy_variables], axis = 1, sort = False)\n",
    "\n",
    "    x31_dummy_variables = pd.get_dummies(df1['x31'], \n",
    "                                         drop_first = True, \n",
    "                                         prefix = 'x31', \n",
    "                                         prefix_sep = '_', \n",
    "                                         dummy_na = True)\n",
    "    \n",
    "    df2 = pd.concat([df2, x31_dummy_variables], axis = 1, sort = False)\n",
    "\n",
    "    x81_dummy_variables = pd.get_dummies(df1['x81'], \n",
    "                                         drop_first = True, \n",
    "                                         prefix = 'x81', \n",
    "                                         prefix_sep = '_', \n",
    "                                         dummy_na = True)\n",
    "\n",
    "    df2 = pd.concat([df2, x81_dummy_variables], axis = 1, sort = False)\n",
    "\n",
    "    x82_dummy_variables = pd.get_dummies(df1['x82'], \n",
    "                                         drop_first = True, \n",
    "                                         prefix = 'x82', \n",
    "                                         prefix_sep = '_', \n",
    "                                         dummy_na = True)\n",
    "\n",
    "    df2 = pd.concat([df2, x82_dummy_variables], axis = 1, sort = False)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "acdd7e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filters out and retrives only those columns previously determined to be the most useful during the\n",
    "    creation of the prediction model\n",
    "'''\n",
    "def filter_df_column_variables(df, ordr_clmn_names_lst):\n",
    "    \n",
    "    necessary_clmn_vars_set = set(ordr_clmn_names_lst)\n",
    "    avlbl_clmn_vars_set = set(df.columns)\n",
    "    \n",
    "    '''\n",
    "    Depending upon the type and amount of data passed in, not all dummy variables will always be\n",
    "        successfully generated, necessitating their inclusion afterwards\n",
    "    '''\n",
    "    if necessary_clmn_vars_set.issubset(avlbl_clmn_vars_set) == False:\n",
    "        nan_df = pd.DataFrame(np.nan, index = range(df.shape[0]), columns = ordr_clmn_names_lst)\n",
    "        df = df.combine_first(nan_df)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "    df = df[ordr_clmn_names_lst].copy(deep = True)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "347842ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transform_input_data_pipeline(json_data, ordr_clmn_names_lst):\n",
    "    \n",
    "    df = transform_json_to_df(json_data)\n",
    "    \n",
    "    #df = drop_df_rows_missing_data(df)\n",
    "    print(df.shape)\n",
    "    \n",
    "    if df.shape[0] <= 1:\n",
    "        df = pd.DataFrame()\n",
    " \n",
    "    else:\n",
    "        df = format_df_column_variables(df)\n",
    "        #print('Format')\n",
    "        #print(df.head())\n",
    "        #print(df.columns)\n",
    "\n",
    "        imputed_df = impute_missing_df_data(df)\n",
    "        #print('Impute')\n",
    "        #print(imputed_df.head())\n",
    "        #print(imputed_df.columns)\n",
    "\n",
    "        scaled_imputed_df = scale_df_data(imputed_df)\n",
    "        #print('Scale')\n",
    "        #print(scaled_imputed_df.head())\n",
    "        #print(scaled_imputed_df.columns)\n",
    "\n",
    "        df = create_df_dummy_column_variables_old(df, scaled_imputed_df)\n",
    "        #print(df.head())\n",
    "        #print(df.columns)\n",
    "\n",
    "        df = filter_df_column_variables(df, ordr_clmn_names_lst)\n",
    "        print('Filter')\n",
    "        print(df.head())\n",
    "        print(df.columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "20cbe27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_outcome(df, model, alphanum_ord_clmn_var_names_lst):\n",
    "    \n",
    "    num_rows_df = df.shape[0]\n",
    "    \n",
    "    if num_rows_df == 0:\n",
    "        return json.dumps({'message': 'Error'})\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        for row in range(num_rows_df):\n",
    "            predicted_outcome = 0\n",
    "            model_inputs = {}\n",
    "\n",
    "            for var in alphanum_ord_clmn_var_names_lst:\n",
    "                model_inputs[var] = df.iloc[row][var]\n",
    "\n",
    "            predicted_probability = model.predict(df.iloc[row])[0]\n",
    "\n",
    "            if predicted_probability >= 0.75:\n",
    "                predicted_outcome = 1\n",
    "\n",
    "            model_predictions = {'business_outcome': str(predicted_outcome), \n",
    "                                 'p_hat': str(predicted_probability)}\n",
    "\n",
    "            output = model_predictions | model_inputs\n",
    "\n",
    "            return json.dumps(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "144f84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    json = sample_json3\n",
    "    \n",
    "    final_df_column_variable_names_order = [\n",
    "        'x5_saturday', 'x81_July', 'x81_December', 'x31_japan', 'x81_October', \n",
    "        'x5_sunday', 'x31_asia', 'x81_February', 'x91', 'x81_May', \n",
    "        'x5_monday', 'x81_September', 'x81_March', 'x53', 'x81_November', \n",
    "        'x44', 'x81_June', 'x12', 'x5_tuesday', 'x81_August', \n",
    "        'x81_January', 'x62', 'x31_germany', 'x58', 'x56']\n",
    "    \n",
    "    alphanumerically_sorted_df_column_variable_names = sorted(final_df_column_variable_names_order)\n",
    "    \n",
    "    mdl = load_model()\n",
    "    \n",
    "    df = extract_transform_input_data_pipeline(json, \n",
    "                                               final_df_column_variable_names_order)\n",
    "    \n",
    "    json_output_message = predict_outcome(df, mdl, alphanumerically_sorted_df_column_variable_names)\n",
    "    \n",
    "    print(json_output_message[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9849db35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042317</td>\n",
       "      <td>-3.344721</td>\n",
       "      <td>4.635124</td>\n",
       "      <td>-0.598396</td>\n",
       "      <td>-0.647772</td>\n",
       "      <td>monday</td>\n",
       "      <td>0.184902</td>\n",
       "      <td>46.690015</td>\n",
       "      <td>3.034132</td>\n",
       "      <td>0.364704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.493304</td>\n",
       "      <td>0.373853</td>\n",
       "      <td>0.941435</td>\n",
       "      <td>3.546798</td>\n",
       "      <td>-99.857488</td>\n",
       "      <td>0.403926</td>\n",
       "      <td>1.653787</td>\n",
       "      <td>0.007715</td>\n",
       "      <td>-32.021646</td>\n",
       "      <td>-60.312783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.033160</td>\n",
       "      <td>-0.340140</td>\n",
       "      <td>5.871823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.122133</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>0.997773</td>\n",
       "      <td>51.581411</td>\n",
       "      <td>1.709219</td>\n",
       "      <td>0.844079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521119</td>\n",
       "      <td>0.148424</td>\n",
       "      <td>0.925301</td>\n",
       "      <td>3.830426</td>\n",
       "      <td>-101.105748</td>\n",
       "      <td>0.055775</td>\n",
       "      <td>0.564890</td>\n",
       "      <td>0.051716</td>\n",
       "      <td>-32.540612</td>\n",
       "      <td>-266.725795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.029367</td>\n",
       "      <td>-3.239301</td>\n",
       "      <td>4.724436</td>\n",
       "      <td>2.211831</td>\n",
       "      <td>0.551611</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>0.492405</td>\n",
       "      <td>87.179042</td>\n",
       "      <td>4.333755</td>\n",
       "      <td>0.513789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154492</td>\n",
       "      <td>-0.034504</td>\n",
       "      <td>0.904042</td>\n",
       "      <td>3.642968</td>\n",
       "      <td>-107.476487</td>\n",
       "      <td>1.046718</td>\n",
       "      <td>1.494123</td>\n",
       "      <td>0.231084</td>\n",
       "      <td>-32.740954</td>\n",
       "      <td>-4.327887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.065676</td>\n",
       "      <td>1.892277</td>\n",
       "      <td>4.818741</td>\n",
       "      <td>0.640313</td>\n",
       "      <td>1.944562</td>\n",
       "      <td>friday</td>\n",
       "      <td>0.208718</td>\n",
       "      <td>73.573314</td>\n",
       "      <td>4.929132</td>\n",
       "      <td>0.116004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305243</td>\n",
       "      <td>-0.099213</td>\n",
       "      <td>0.712234</td>\n",
       "      <td>3.853489</td>\n",
       "      <td>-91.650053</td>\n",
       "      <td>0.499861</td>\n",
       "      <td>2.804358</td>\n",
       "      <td>0.627921</td>\n",
       "      <td>-32.190043</td>\n",
       "      <td>103.192597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.357126</td>\n",
       "      <td>-1.852161</td>\n",
       "      <td>5.367849</td>\n",
       "      <td>-0.069869</td>\n",
       "      <td>-0.641455</td>\n",
       "      <td>saturday</td>\n",
       "      <td>0.940286</td>\n",
       "      <td>72.773335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.191044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617258</td>\n",
       "      <td>0.307445</td>\n",
       "      <td>0.376738</td>\n",
       "      <td>3.306958</td>\n",
       "      <td>-99.557140</td>\n",
       "      <td>1.275527</td>\n",
       "      <td>1.476482</td>\n",
       "      <td>0.122798</td>\n",
       "      <td>-32.957087</td>\n",
       "      <td>-111.509168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0  0.042317 -3.344721  4.635124 -0.598396 -0.647772    monday  0.184902   \n",
       "1 -1.033160 -0.340140  5.871823       NaN  0.122133   tuesday  0.997773   \n",
       "2  2.029367 -3.239301  4.724436  2.211831  0.551611   tuesday  0.492405   \n",
       "3 -0.065676  1.892277  4.818741  0.640313  1.944562    friday  0.208718   \n",
       "4 -0.357126 -1.852161  5.367849 -0.069869 -0.641455  saturday  0.940286   \n",
       "\n",
       "          x7        x8        x9  ...       x90       x91       x92       x93  \\\n",
       "0  46.690015  3.034132  0.364704  ... -0.493304  0.373853  0.941435  3.546798   \n",
       "1  51.581411  1.709219  0.844079  ...  0.521119  0.148424  0.925301  3.830426   \n",
       "2  87.179042  4.333755  0.513789  ...  0.154492 -0.034504  0.904042  3.642968   \n",
       "3  73.573314  4.929132  0.116004  ...  0.305243 -0.099213  0.712234  3.853489   \n",
       "4  72.773335       NaN  0.191044  ...  0.617258  0.307445  0.376738  3.306958   \n",
       "\n",
       "          x94       x95       x96       x97        x98         x99  \n",
       "0  -99.857488  0.403926  1.653787  0.007715 -32.021646  -60.312783  \n",
       "1 -101.105748  0.055775  0.564890  0.051716 -32.540612 -266.725795  \n",
       "2 -107.476487  1.046718  1.494123  0.231084 -32.740954   -4.327887  \n",
       "3  -91.650053  0.499861  2.804358  0.627921 -32.190043  103.192597  \n",
       "4  -99.557140  1.275527  1.476482  0.122798 -32.957087 -111.509168  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_testing_data1 = pd.read_csv(os.path.join('../training', 'exercise_26_test.csv'))\n",
    "raw_testing_data2 = pd.read_csv(os.path.join('../training', 'exercise_26_test.csv'), nrows = 1)\n",
    "raw_testing_data3 = pd.read_csv(os.path.join('../training', 'exercise_26_test.csv'), nrows = 10)\n",
    "raw_testing_data4 = pd.read_csv(os.path.join('../training', 'exercise_26_test.csv'), nrows = 100)\n",
    "raw_testing_data5 = pd.read_csv(os.path.join('../training', 'exercise_26_test.csv'), nrows = 1000)\n",
    "\n",
    "sample_json1 = raw_testing_data1.to_json(orient = 'records')\n",
    "sample_json2 = raw_testing_data2.to_json(orient = 'records')\n",
    "sample_json3 = raw_testing_data3.to_json(orient = 'records')\n",
    "sample_json4 = raw_testing_data4.to_json(orient = 'records')\n",
    "sample_json5 = raw_testing_data5.to_json(orient = 'records')\n",
    "\n",
    "#raw_testing_data1.head()\n",
    "#raw_testing_data2.head()\n",
    "raw_testing_data3.head()\n",
    "#raw_testing_data4.head()\n",
    "#raw_testing_data5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d7f5044f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100)\n",
      "Filter\n",
      "   x5_saturday  x81_July  x81_December  x31_japan  x81_October  x5_sunday  \\\n",
      "0            0         0           0.0        0.0            1          0   \n",
      "1            0         0           0.0        0.0            0          0   \n",
      "2            0         1           0.0        0.0            0          0   \n",
      "3            0         0           0.0        0.0            1          0   \n",
      "4            1         0           0.0        0.0            0          0   \n",
      "\n",
      "   x31_asia  x81_February       x91  x81_May  ...       x44  x81_June  \\\n",
      "0         0             0  0.493532        0  ... -1.437192         0   \n",
      "1         0             0 -0.323474        0  ...  0.047566         0   \n",
      "2         0             0 -0.986448        0  ...  1.550309         0   \n",
      "3         0             0 -1.220968        0  ... -0.669170         0   \n",
      "4         0             0  0.252854        1  ...  1.446686         0   \n",
      "\n",
      "        x12  x5_tuesday  x81_August  x81_January       x62  x31_germany  \\\n",
      "0  0.880425           0         0.0          0.0 -0.368322            1   \n",
      "1 -0.919315           1         0.0          0.0  0.421871            0   \n",
      "2  0.875194           1         0.0          0.0  1.109902            1   \n",
      "3 -0.750223           0         0.0          0.0  1.795678            1   \n",
      "4  1.039567           0         0.0          0.0  1.222949            0   \n",
      "\n",
      "        x58       x56  \n",
      "0 -0.206565 -1.756735  \n",
      "1  0.249949  1.786277  \n",
      "2  0.200657 -0.443787  \n",
      "3 -0.768771  0.981128  \n",
      "4  1.921654  0.643108  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Index(['x5_saturday', 'x81_July', 'x81_December', 'x31_japan', 'x81_October',\n",
      "       'x5_sunday', 'x31_asia', 'x81_February', 'x91', 'x81_May', 'x5_monday',\n",
      "       'x81_September', 'x81_March', 'x53', 'x81_November', 'x44', 'x81_June',\n",
      "       'x12', 'x5_tuesday', 'x81_August', 'x81_January', 'x62', 'x31_germany',\n",
      "       'x58', 'x56'],\n",
      "      dtype='object')\n",
      "{\"business_outcome\": \"0\", \"p_hat\": \"0.41928032480876437\", \"x12\": 0.8804246375433493, \"x31_asia\": 0.0, \"x31_germany\": 1.0, \"x31_japan\": 0.0, \"x44\": -1.4371920774078044, \"x53\": 1.3984395851484266, \"x56\": -1.7567352771898606, \"x58\": -0.20656541935803377, \"x5_monday\": 1.0, \"x5_saturday\": 0.0, \"x5_sunday\": 0.0, \"x5_tuesday\": 0.0, \"x62\": -0.3683218747220857, \"x81_August\": 0.0, \"x81_December\": 0.0, \"x81_February\": 0.0, \"x81_January\": 0.0, \"x81_July\": 0.0, \"x81_June\": 0.0, \"x81_March\": 0.0, \"x81_May\": 0.0, \"x81_November\": 0.0, \"x81_October\": 1.0, \"x81_September\": 0.0, \"x91\": 0.4935320292349516}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/dtjmhvc55bd1swsb_wk1cnbr0000gn/T/ipykernel_54384/1612771863.py:4: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['x12'] = df['x12'].str.replace('$', '')\n",
      "/var/folders/tc/dtjmhvc55bd1swsb_wk1cnbr0000gn/T/ipykernel_54384/1612771863.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['x12'] = df['x12'].str.replace(')', '')\n",
      "/var/folders/tc/dtjmhvc55bd1swsb_wk1cnbr0000gn/T/ipykernel_54384/1612771863.py:7: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['x12'] = df['x12'].str.replace('(', '-')\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b3a175e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "75b8cb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153148e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
