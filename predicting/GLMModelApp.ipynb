{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a35dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Required Libraries\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi import Request\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e87b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    '''\n",
    "    Returns loaded in, pre-trained Prediction, SimpleImputer and StandardScalar models\n",
    "    '''\n",
    "    \n",
    "    mdl = sm.load('../models/glm_final_model.pickle')\n",
    "    si = pickle.load(open('../models/glm_simple_imputer.pickle', 'rb'))\n",
    "    ss = pickle.load(open('../models/glm_standard_scalar.pickle', 'rb'))\n",
    "    \n",
    "    return mdl, si, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b6360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_json_to_df(obj):\n",
    "    '''\n",
    "    Returns a DataFrame containing raw JSON data\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    obj -- Raw JSON data in the data type form of a String, List or Dictionary\n",
    "    '''\n",
    "    \n",
    "    if isinstance(obj, str):\n",
    "        if obj[0] == '[' and obj[len(obj) - 1] == ']':\n",
    "            df = pd.read_json(obj, orient = 'records')\n",
    "        elif obj[0] == '{' and obj[len(obj) - 1] == '}':\n",
    "            obj = '[' + obj + ']'\n",
    "            df = pd.read_json(obj, orient = 'records')\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "    elif isinstance(obj, list):\n",
    "        df = pd.DataFrame.from_dict(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        obj = [obj]\n",
    "        df = pd.DataFrame.from_dict(obj)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e0e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_df(max_batch_size, df):\n",
    "    '''\n",
    "    Returns\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    max_batch_size -- \n",
    "    df -- \n",
    "    '''\n",
    "    \n",
    "    df_batches = []\n",
    "    \n",
    "    num_rows = len(df.index)\n",
    "    \n",
    "    if num_rows > max_batch_size:\n",
    "        \n",
    "        num_batches = ceil(num_rows / max_batch_size)\n",
    "        \n",
    "        for batch_num in range(num_batches): \n",
    "            starting_index = batch_num * max_batch_size\n",
    "            #print(f'Starting Index: {starting_index}')\n",
    "            \n",
    "            if batch_num != num_batches - 1:\n",
    "                ending_index = (batch_num + 1) * max_batch_size\n",
    "                #print(f'Ending Index: {ending_index}')\n",
    "                batch_df = df.iloc[starting_index:ending_index].copy(deep = True)\n",
    "            \n",
    "            else:\n",
    "                batch_df = df.iloc[starting_index:].copy(deep = True)\n",
    "                \n",
    "            df_batches.append(batch_df)\n",
    "    \n",
    "    else:\n",
    "        df_batches.append(df.copy(deep = True))\n",
    "        \n",
    "    return df_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411ecffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_column_variables(df):\n",
    "    '''\n",
    "    Returns a DataFrame after having transformed those columns, whose values \n",
    "        consisted of those of the String data type and representing quantitative \n",
    "        data, into values being that of the Float data type after removing any\n",
    "        non mathematically interpretable symbols \n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- A DataFrame containing raw JSON data\n",
    "    '''\n",
    "    \n",
    "    # Formats the 'x12' and 'x63' columns', consisting of the String data type, \n",
    "    #    and respectively representing first monetary then percentage values,\n",
    "    #    into the Float data type so as to be able to later on apply mathematical\n",
    "    #    work upon said columns' values\n",
    "    \n",
    "    columns_to_format = ['x12', 'x63']\n",
    "    \n",
    "    unwanted_symbols = ['$', ',', '(', ')', '%']\n",
    "    \n",
    "    for clmn in columns_to_format:\n",
    "        for usymb in unwanted_symbols:\n",
    "            if usymb != '(':\n",
    "                df[clmn] = df[clmn].str.replace(usymb, '', regex = True)\n",
    "            else:\n",
    "                df[clmn] = df[clmn].str.replace(usymb, '-', regex = True)\n",
    "        df[clmn] = df[clmn].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc361ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_df_data(si, df):\n",
    "    '''\n",
    "    Returns a DataFrame with no column variables whose data is of a qualitative nature, \n",
    "        as well as having filled in any remaining blank, NaN, NULL or otherwise missing \n",
    "        values via the usage of an imported, pre-trained SimpleImputer using a mean based \n",
    "        strategy\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    df -- A formatted DataFrame\n",
    "    '''\n",
    "    \n",
    "    # Columns containing qualitative data that are dropped ahead of the imputation phase\n",
    "    #    as the SimpleImputer cannot properly function upon them\n",
    "    qual_clmns = ['x5', 'x31', 'x81', 'x82']\n",
    "\n",
    "    df = pd.DataFrame(si.transform(df.drop(columns = qual_clmns)), \n",
    "                      columns = df.drop(columns = qual_clmns).columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3211268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df_data(ss, df):\n",
    "    '''\n",
    "    Returns a DataFrame whose column variable values have all been scaled via a \n",
    "        standardization method for the purpose of feature scaling, utilizing an\n",
    "        imported, pre-trained StandardScalar\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    df -- A formatted DataFrame without any blank, NaN, NULL or otherwise missing values\n",
    "    '''\n",
    "    \n",
    "    # Of particular interest and focus is that of the 'x12' column representing\n",
    "    #    monetary values which tend to outscale all other column variable values\n",
    "    #    by some orders of magnitude\n",
    "    \n",
    "    df = pd.DataFrame(ss.transform(df), \n",
    "                      columns = df.columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b65d9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_dummy_column_variables_new(df1, df2):\n",
    "    '''\n",
    "    Returns a DataFrame with dummy variables for those column variables consisting of\n",
    "        qualitative data, as well as whose column variables of numeric quantitative data\n",
    "        have no missing values and are scaled\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    df1 -- A DataFrame containing the original raw JSON data in order to retrieve those column\n",
    "           variable values of a qualitative nature previously dropped and must now be \n",
    "           dummified\n",
    "    df2 -- A DataFrame with no missing column variable values and whose said values have already been scaled\n",
    "    '''\n",
    "    \n",
    "    # A list of column variable names of a quantitative nature which require dummification\n",
    "    vars_to_dummify = ['x5', 'x31', 'x81', 'x82']\n",
    "    dummy_dfs = [df2]\n",
    "    \n",
    "    for var in vars_to_dummify:\n",
    "\n",
    "        var_dummy_vars_df = pd.get_dummies(df1[var], \n",
    "                                        drop_first = True, \n",
    "                                        prefix = var, \n",
    "                                        prefix_sep = '_', \n",
    "                                        dummy_na = True)\n",
    "        \n",
    "        dummy_dfs.append(var_dummy_vars_df)\n",
    "\n",
    "    df2 = pd.concat(dummy_dfs, axis = 1, sort = False)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8f337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_column_variables(ordr_clmn_names_lst, df):\n",
    "    '''\n",
    "    Returns a DataFrame containing only those column variables required by the pre-trained model\n",
    "        for predictions, filtering out from the given DataFrame only said columns\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    ordr_clmn_names_lst -- A list of pre-ordered DataFrame column variable names required by the pre-trained model\n",
    "    df -- The DataFrame with dummy variables, and whose quantitative values have been scaled and \n",
    "          have none missing\n",
    "    '''\n",
    "    \n",
    "    necessary_clmn_vars_set = set(ordr_clmn_names_lst)\n",
    "    avlbl_clmn_vars_set = set(df.columns)\n",
    "\n",
    "    # Depending upon the type and amount of JSON data originally having been passed in, \n",
    "    #    not all of the desired dummy variables will always be successfully generated, \n",
    "    #    necessitating their inclusion afterwards via the code below\n",
    "    if necessary_clmn_vars_set.issubset(avlbl_clmn_vars_set) == False:\n",
    "        nan_df = pd.DataFrame(np.nan, index = range(df.shape[0]), columns = ordr_clmn_names_lst)\n",
    "        df = df.combine_first(nan_df)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "    #df = df[ordr_clmn_names_lst].copy()\n",
    "    df = df[ordr_clmn_names_lst].copy(deep = True)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38021796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_outcomes(df, mdl, alphanum_ord_clmn_var_names_lst):\n",
    "    '''\n",
    "    Returns a JSON message containing either the model's predicted outcomes, \n",
    "        marked as 'business_outcome', predicted probability, marked as 'p_hat',\n",
    "        and the inputs in the alphanumerical order of their variables' names or,\n",
    "        should the predicted probability be under that of 75%, a message indicating\n",
    "        as such; if no valid JSON data as far as the application can tell is passed in, \n",
    "        then a JSON encoded error message will be returned instead\n",
    "        \n",
    "    Keyword Arguments:\n",
    "    df -- A DataFrame containing only those 25 ordered column variables required by the pre-trained model\n",
    "    mdl -- A pre-trained, imported prediction model\n",
    "    alphanum_ord_clmn_var_names_lst -- A list of alphanumerically ordered DataFrame column variable names \n",
    "                                       required by the pre-trained model\n",
    "    '''\n",
    "    \n",
    "    num_rows_df = df.shape[0]\n",
    "    \n",
    "    if num_rows_df == 0:\n",
    "        \n",
    "        return {'message': 'ERROR - No valid JSON data available for prediction.'}\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        output_msgs_lst = []\n",
    "    \n",
    "        for row in range(num_rows_df):\n",
    "            predicted_outcome = 0\n",
    "            mdl_inputs = {}\n",
    "\n",
    "            predicted_probability = mdl.predict(df.iloc[row])[0]\n",
    "\n",
    "            if predicted_probability >= 0.75:\n",
    "                predicted_outcome = 1\n",
    "                \n",
    "                for var in alphanum_ord_clmn_var_names_lst:\n",
    "                    mdl_inputs[var] = df.iloc[row][var]\n",
    "\n",
    "                mdl_predictions = {'business_outcome': str(predicted_outcome), \n",
    "                                    'p_hat': str(predicted_probability)}\n",
    "\n",
    "                prediction_msg = mdl_predictions | mdl_inputs\n",
    "\n",
    "                output_msgs_lst.append(prediction_msg)\n",
    "\n",
    "            else:\n",
    "                #output_msgs_lst.append({'message': 'Business outcome probability too low.'})\n",
    "                pass\n",
    "        \n",
    "        return output_msgs_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63c64c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transform_predict_df(df, si, ss, ordr_clmn_names_lst, \n",
    "                                 mdl, alphanum_ord_clmn_var_names_lst):\n",
    "    '''\n",
    "    Returns \n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- A DataFrame containing raw JSON data\n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    ordr_clmn_names_lst -- A list of pre-ordered DataFrame column variable names required by the pre-trained model\n",
    "    mdl -- A pre-trained, imported prediction model\n",
    "    alphanum_ord_clmn_var_names_lst -- A list of alphanumerically ordered DataFrame column variable names \n",
    "                                       required by the pre-trained model\n",
    "    '''\n",
    "    \n",
    "    if df.shape[0] < 1:\n",
    "        filtered_dummy_scaled_imputed_df = pd.DataFrame()\n",
    " \n",
    "    else:\n",
    "        \n",
    "        # Resets the index to avoid concatenation issues with dummy variable DataFrames\n",
    "        df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        batch_df = df.copy(deep = True)\n",
    "        \n",
    "        formatted_df = format_df_column_variables(batch_df)\n",
    "\n",
    "        imputed_df = impute_missing_df_data(si, formatted_df)\n",
    "\n",
    "        scaled_imputed_df = scale_df_data(ss, imputed_df)\n",
    "\n",
    "\n",
    "        dummy_scaled_imputed_df = create_df_dummy_column_variables_new(df, scaled_imputed_df)\n",
    "\n",
    "\n",
    "        filtered_dummy_scaled_imputed_df = filter_df_column_variables(ordr_clmn_names_lst, \n",
    "                                                                    dummy_scaled_imputed_df)\n",
    "        \n",
    "    predictions = predict_outcomes(filtered_dummy_scaled_imputed_df, mdl, \n",
    "                                   alphanum_ord_clmn_var_names_lst)\n",
    "                                      \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d880874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_extract_transform_predict_df(df, si, ss, ordr_clmn_names_lst, \n",
    "                                             mdl, alphanum_ord_clmn_var_names_lst):\n",
    "    '''\n",
    "    Returns \n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df -- A DataFrame containing raw JSON data\n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    ordr_clmn_names_lst -- A list of pre-ordered DataFrame column variable names required by the pre-trained model\n",
    "    mdl -- A pre-trained, imported prediction model\n",
    "    alphanum_ord_clmn_var_names_lst -- A list of alphanumerically ordered DataFrame column variable names \n",
    "                                       required by the pre-trained model\n",
    "    '''\n",
    "    \n",
    "    if df.shape[0] < 1:\n",
    "        filtered_dummy_scaled_imputed_df = pd.DataFrame()\n",
    " \n",
    "    else:\n",
    "        \n",
    "        # Resets the index to avoid concatenation issues with dummy variable DataFrames\n",
    "        df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        batch_df = df.copy(deep = True)\n",
    "        \n",
    "        formatted_df = format_df_column_variables(batch_df)\n",
    "\n",
    "        imputed_df = impute_missing_df_data(si, formatted_df)\n",
    "\n",
    "        scaled_imputed_df = scale_df_data(ss, imputed_df)\n",
    "\n",
    "\n",
    "        dummy_scaled_imputed_df = create_df_dummy_column_variables_new(df, scaled_imputed_df)\n",
    "\n",
    "\n",
    "        filtered_dummy_scaled_imputed_df = filter_df_column_variables(ordr_clmn_names_lst, \n",
    "                                                                    dummy_scaled_imputed_df)\n",
    "        \n",
    "    predictions = predict_outcomes(filtered_dummy_scaled_imputed_df, mdl, \n",
    "                                   alphanum_ord_clmn_var_names_lst)\n",
    "                                      \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0733ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transform_predict_df_batches(df_batches, si, ss, ordr_clmn_names_lst, \n",
    "                                         mdl, alphanum_ord_clmn_var_names_lst):\n",
    "    '''\n",
    "    Returns\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df_batches -- \n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    ordr_clmn_names_lst -- A list of DataFrame column variable names required by the pre-trained model\n",
    "    mdl -- A pre-trained, imported prediction model\n",
    "    alphanum_ord_clmn_var_names_lst -- A list of alphanumerically ordered DataFrame column variable names \n",
    "                                       required by the pre-trained model\n",
    "    '''\n",
    "    \n",
    "    df_batches_predictions = []\n",
    "    \n",
    "    for batch in df_batches:\n",
    "        batch_predictions = extract_transform_predict_df(batch, si, ss, ordr_clmn_names_lst, \n",
    "                                                         mdl, alphanum_ord_clmn_var_names_lst)\n",
    "        \n",
    "        df_batches_predictions += batch_predictions\n",
    "        \n",
    "    return df_batches_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b05e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_extract_transform_predict_df_batches(df_batches, si, ss, ordr_clmn_names_lst, \n",
    "                                                     mdl, alphanum_ord_clmn_var_names_lst):\n",
    "    '''\n",
    "    Returns\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    df_batches -- \n",
    "    si -- A pre-trained, imported SimpleImputer\n",
    "    ss -- A pre-trained, imported StandardScalar\n",
    "    ordr_clmn_names_lst -- A list of DataFrame column variable names required by the pre-trained model\n",
    "    mdl -- A pre-trained, imported prediction model\n",
    "    alphanum_ord_clmn_var_names_lst -- A list of alphanumerically ordered DataFrame column variable names \n",
    "                                       required by the pre-trained model\n",
    "    '''\n",
    "    \n",
    "    df_batches_predictions = []\n",
    "    \n",
    "    tasks = []\n",
    "    \n",
    "    for batch in df_batches:\n",
    "        tasks.append(asyncio.ensure_future(async_extract_transform_predict_df(\n",
    "            batch, si, ss, ordr_clmn_names_lst, mdl, alphanum_ord_clmn_var_names_lst)))\n",
    "                                           \n",
    "    results = await asyncio.gather(*tasks)\n",
    "                                           \n",
    "    for batch_predictions in results:\n",
    "        df_batches_predictions += batch_predictions\n",
    "        \n",
    "    return df_batches_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f476de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rows_per_batch, input_json):\n",
    "    '''\n",
    "    Returns a single event or a list of events as a JSON message containing the business outcome, \n",
    "        probability of said outcome, along with the input variables which led to said outcome \n",
    "        in alphanumerical order for all those predictions which met the minimum standard of 75% \n",
    "        chance of a successful sale to a potential buying customer\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    input_json -- Raw JSON data\n",
    "    '''\n",
    "    \n",
    "    # List of the already properly ordered column variables required by the pre-trained\n",
    "    #    model in order for it to carry out accurate predictions\n",
    "    final_df_column_variable_names_order = [\n",
    "        'x5_saturday', 'x81_July', 'x81_December', 'x31_japan', 'x81_October', \n",
    "        'x5_sunday', 'x31_asia', 'x81_February', 'x91', 'x81_May', \n",
    "        'x5_monday', 'x81_September', 'x81_March', 'x53', 'x81_November', \n",
    "        'x44', 'x81_June', 'x12', 'x5_tuesday', 'x81_August', \n",
    "        'x81_January', 'x62', 'x31_germany', 'x58', 'x56']\n",
    "    \n",
    "    alphanumerically_sorted_df_column_variable_names = sorted(final_df_column_variable_names_order)\n",
    "    \n",
    "    mdl, si, ss = load_models()\n",
    "    \n",
    "    df = transform_json_to_df(input_json)\n",
    "    \n",
    "    df_batches = batch_df(rows_per_batch, df)\n",
    "    \n",
    "    starting_time = time.time()\n",
    "    \n",
    "    json_output_message = extract_transform_predict_df_batches(df_batches, \n",
    "                                                               si, \n",
    "                                                               ss, \n",
    "                                                               final_df_column_variable_names_order, \n",
    "                                                               mdl, \n",
    "                                                               alphanumerically_sorted_df_column_variable_names)\n",
    "    \n",
    "    print(f'Time To Completion: {time.time() - starting_time} Seconds \\n')\n",
    "\n",
    "    return json_output_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc47f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_main(rows_per_batch, input_json):\n",
    "    '''\n",
    "    Returns a single event or a list of events as a JSON message containing the business outcome, \n",
    "        probability of said outcome, along with the input variables which led to said outcome \n",
    "        in alphanumerical order for all those predictions which met the minimum standard of 75% \n",
    "        chance of a successful sale to a potential buying customer\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    input_json -- Raw JSON data\n",
    "    '''\n",
    "    \n",
    "    # List of the already properly ordered column variables required by the pre-trained\n",
    "    #    model in order for it to carry out accurate predictions\n",
    "    final_df_column_variable_names_order = [\n",
    "        'x5_saturday', 'x81_July', 'x81_December', 'x31_japan', 'x81_October', \n",
    "        'x5_sunday', 'x31_asia', 'x81_February', 'x91', 'x81_May', \n",
    "        'x5_monday', 'x81_September', 'x81_March', 'x53', 'x81_November', \n",
    "        'x44', 'x81_June', 'x12', 'x5_tuesday', 'x81_August', \n",
    "        'x81_January', 'x62', 'x31_germany', 'x58', 'x56']\n",
    "    \n",
    "    alphanumerically_sorted_df_column_variable_names = sorted(final_df_column_variable_names_order)\n",
    "    \n",
    "    mdl, si, ss = load_models()\n",
    "    \n",
    "    df = transform_json_to_df(input_json)\n",
    "    \n",
    "    df_batches = batch_df(rows_per_batch, df)\n",
    "    \n",
    "    starting_time = time.time()\n",
    "    \n",
    "    json_output_message = await async_extract_transform_predict_df_batches(\n",
    "        df_batches, si, ss, final_df_column_variable_names_order, \n",
    "        mdl, alphanumerically_sorted_df_column_variable_names)\n",
    "    \n",
    "    print(f'Time To Completion: {time.time() - starting_time} Seconds \\n')\n",
    "\n",
    "    return json_output_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e57a5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Debugging & Testing Purposes\n",
    "raw_testing_data1 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'))\n",
    "\n",
    "raw_testing_data2 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 1)\n",
    "raw_testing_data3 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 10)\n",
    "raw_testing_data4 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 100)\n",
    "raw_testing_data5 = pd.read_csv(os.path.join('../testing', 'exercise_26_test.csv'), nrows = 1000)\n",
    "\n",
    "#raw_testing_data1.head()\n",
    "#raw_testing_data2.head()\n",
    "#raw_testing_data3.head()\n",
    "#raw_testing_data4.head()\n",
    "#raw_testing_data5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468a332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_raw_json_10000_rows = raw_testing_data1.to_json(orient = 'records')\n",
    "\n",
    "sample_raw_json_1_row_v2 = raw_testing_data2.to_json(orient = 'records')\n",
    "sample_raw_json_10_rows = raw_testing_data3.to_json(orient = 'records')\n",
    "sample_raw_json_100_rows = raw_testing_data4.to_json(orient = 'records')\n",
    "sample_raw_json_1000_rows = raw_testing_data5.to_json(orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6d4f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_raw_json_1_row_v1 = \"{\\\"x0\\\":0.042317,\\\"x1\\\":-3.344721,\\\"x2\\\":4.6351242122,\\\"x3\\\":-0.5983959993,\\\"x4\\\":-0.6477715046,\\\"x5\\\":\\\"monday\\\",\\\"x6\\\":0.184902,\\\"x7\\\":46.690015,\\\"x8\\\":3.034132,\\\"x9\\\":0.364704,\\\"x10\\\":14.260733,\\\"x11\\\":-1.559332,\\\"x12\\\":\\\"$5,547.78\\\",\\\"x13\\\":0.520324,\\\"x14\\\":31.212255,\\\"x15\\\":4.891671,\\\"x16\\\":0.357763,\\\"x17\\\":14.766366,\\\"x18\\\":-17.467243,\\\"x19\\\":0.224628,\\\"x20\\\":0.096752,\\\"x21\\\":1.305564,\\\"x22\\\":0.353632,\\\"x23\\\":3.909028,\\\"x24\\\":-91.273052,\\\"x25\\\":1.396952,\\\"x26\\\":4.401593,\\\"x27\\\":0.443086,\\\"x28\\\":14.048787,\\\"x29\\\":-0.932243,\\\"x30\\\":5.255472,\\\"x31\\\":\\\"germany\\\",\\\"x32\\\":0.54199153,\\\"x33\\\":2.98948039,\\\"x34\\\":-1.78334189,\\\"x35\\\":0.80127315,\\\"x36\\\":-2.60231221,\\\"x37\\\":3.39682926,\\\"x38\\\":-1.22322646,\\\"x39\\\":-2.20977636,\\\"x40\\\":-68.69,\\\"x41\\\":522.25,\\\"x42\\\":-428.69,\\\"x43\\\":381.37,\\\"x44\\\":0.0197503,\\\"x45\\\":0.75116479,\\\"x46\\\":0.8630479008,\\\"x47\\\":-1.0383166613,\\\"x48\\\":-0.2726187635,\\\"x49\\\":-0.3430207259,\\\"x50\\\":0.3109008666,\\\"x51\\\":-0.797841974,\\\"x52\\\":-2.0390175153,\\\"x53\\\":0.87182889,\\\"x54\\\":0.14373012,\\\"x55\\\":-1.15212514,\\\"x56\\\":-2.1703139704,\\\"x57\\\":-0.267842962,\\\"x58\\\":0.212110633,\\\"x59\\\":1.6926559407,\\\"x60\\\":-0.9522767913,\\\"x61\\\":-0.8625864974,\\\"x62\\\":0.0748487158,\\\"x63\\\":\\\"36.29%\\\",\\\"x64\\\":3.47125327,\\\"x65\\\":-3.16656509,\\\"x66\\\":0.65446814,\\\"x67\\\":14.60067029,\\\"x68\\\":-20.57521013,\\\"x69\\\":0.71083785,\\\"x70\\\":0.16983767,\\\"x71\\\":0.55082127,\\\"x72\\\":0.62814576,\\\"x73\\\":3.38608078,\\\"x74\\\":-112.45263714,\\\"x75\\\":1.48370808,\\\"x76\\\":1.77035368,\\\"x77\\\":0.75702363,\\\"x78\\\":14.75731742,\\\"x79\\\":-0.62550355,\\\"x80\\\":null,\\\"x81\\\":\\\"October\\\",\\\"x82\\\":\\\"Female\\\",\\\"x83\\\":-0.7116680715,\\\"x84\\\":-0.2653559892,\\\"x85\\\":0.5175495907,\\\"x86\\\":-1.0881027092,\\\"x87\\\":-1.8188638198,\\\"x88\\\":-1.3584469527,\\\"x89\\\":-0.654995195,\\\"x90\\\":-0.4933042262,\\\"x91\\\":0.373853,\\\"x92\\\":0.94143481,\\\"x93\\\":3.54679834,\\\"x94\\\":-99.8574882,\\\"x95\\\":0.403926,\\\"x96\\\":1.65378726,\\\"x97\\\":0.00771459,\\\"x98\\\":-32.02164582,\\\"x99\\\":-60.3127828}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "995b342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves raw JSON data files for proper testing when Docker becomes involved\n",
    "#with open(os.path.join('../testing', 'sample_raw_json_1_row_v1.json'), 'w') as file:\n",
    "#    file.write(sample_raw_json_1_row_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4d58dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves slices of raw testing data of various sizes whilst \n",
    "#    converting and saving them into a JSON format\n",
    "#sample_raw_json_10000_rows = raw_testing_data1.to_json(os.path.join('../testing', 'sample_raw_json_10000_rows.json'), orient = 'records')\n",
    "\n",
    "#sample_raw_json_1_row_v2 = raw_testing_data2.to_json(os.path.join('../testing', 'sample_raw_json_1_row_v2.json'), orient = 'records')\n",
    "#sample_raw_json_10_rows = raw_testing_data3.to_json(os.path.join('../testing', 'sample_raw_json_10_rows.json'), orient = 'records')\n",
    "#sample_raw_json_100_rows = raw_testing_data4.to_json(os.path.join('../testing', 'sample_raw_json_100_rows.json'), orient = 'records')\n",
    "#sample_raw_json_1000_rows = raw_testing_data5.to_json(os.path.join('../testing', 'sample_raw_json_1000_rows.json'), orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25f5145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_details(json_res, print_res):\n",
    "    print(f'Results Type: {type(json_res)}\\n')\n",
    "    print(f'Results Length: {len(json_res)}\\n')\n",
    "    \n",
    "    if isinstance(json_res, list) and print_res == True:\n",
    "        for result in json_res:\n",
    "            print(f'{result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a5c648a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time To Completion: 0.014929056167602539 Seconds \n",
      "\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 0\n",
    "json_results1 = main(500, sample_raw_json_1_row_v1)\n",
    "results_details(json_results1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "080cfd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time To Completion: 0.01531982421875 Seconds \n",
      "\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 0\n",
    "json_results2 = main(500, sample_raw_json_1_row_v2)\n",
    "results_details(json_results2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fe4e7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time To Completion: 0.019026756286621094 Seconds \n",
      "\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 2\n",
      "\n",
      "{'business_outcome': '1', 'p_hat': '0.8227526744741207', 'x12': -0.9529553121992512, 'x31_asia': 0.0, 'x31_germany': 0.0, 'x31_japan': 0.0, 'x44': -0.018906143399875407, 'x53': -1.5974242627501094, 'x56': 1.6483959900371876, 'x58': 0.599511719639024, 'x5_monday': 0.0, 'x5_saturday': 0.0, 'x5_sunday': 0.0, 'x5_tuesday': 1.0, 'x62': 0.7274434380391581, 'x81_August': 0.0, 'x81_December': 0.0, 'x81_February': 0.0, 'x81_January': 0.0, 'x81_July': 0.0, 'x81_June': 0.0, 'x81_March': 0.0, 'x81_May': 0.0, 'x81_November': 1.0, 'x81_October': 0.0, 'x81_September': 0.0, 'x91': -0.3633580994460479}\n",
      "\n",
      "{'business_outcome': '1', 'p_hat': '0.7539305190612656', 'x12': 1.1180442609776802, 'x31_asia': 0.0, 'x31_germany': 1.0, 'x31_japan': 0.0, 'x44': 1.0491037054747567, 'x53': -1.0452443300416803, 'x56': -0.9224261424370558, 'x58': 0.41258044127697047, 'x5_monday': 0.0, 'x5_saturday': 0.0, 'x5_sunday': 0.0, 'x5_tuesday': 0.0, 'x62': -0.29478102388959054, 'x81_August': 0.0, 'x81_December': 0.0, 'x81_February': 0.0, 'x81_January': 0.0, 'x81_July': 0.0, 'x81_June': 0.0, 'x81_March': 0.0, 'x81_May': 0.0, 'x81_November': 0.0, 'x81_October': 0.0, 'x81_September': 1.0, 'x91': 1.2247713571905237}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 2\n",
    "json_results3 = main(500, sample_raw_json_10_rows)\n",
    "results_details(json_results3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6245668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time To Completion: 0.03585100173950195 Seconds \n",
      "\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 24\n",
    "json_results4 = main(500, sample_raw_json_100_rows)\n",
    "results_details(json_results4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfde8139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time To Completion: 0.29166483879089355 Seconds \n",
      "\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 215\n",
    "json_results5 = main(500, sample_raw_json_1000_rows)\n",
    "results_details(json_results5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1b34fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time To Completion: 3.0024170875549316 Seconds \n",
      "\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 2013\n",
    "json_results6 = main(100, sample_raw_json_10000_rows)\n",
    "results_details(json_results6, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40f69e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time To Completion: 2.9934258460998535 Seconds \n",
      "\n",
      "Results Type: <class 'list'>\n",
      "\n",
      "Results Length: 2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Length: Expected = 2013\n",
    "json_results7 = await async_main(100, sample_raw_json_10000_rows)\n",
    "results_details(json_results7, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba5b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
